\chapter{Simulation}
\section{Implementation}
A ... was implemented to simulate range measurements of rigid bodies. 
Main aspects of simulation: rigid body motion, objects, sensor/measurement, noise, observer
Time: 1 step for every sensor measurement
Data: sensor state, environment state - compute every step

\subsection{Rigid Body Motion}
To simulate range measurements the pose of the sensor and the objects comprising the environment must be computed at each time step. The computations required to do so can be reduced by taking into account the kinds of motion that must be simulated.

The observer actually computes the \textit{relative} position between the sensor and cube and simply uses knowledge of the sensor pose to determine the pose of the cube in the inertial frame. There is no need to simulate complex sensor motions because the motion of the cube can be adjusted to achieve the same result. The only requirement of the sensor motion is that a large field of view is acquired so that the entire target object can be viewed. The scanning behaviour of the sensor is to rotate back and forth about the $z$-axis of the body fixed frame $\{A\}$. To provide a rectangular field of view, the motion of the sensor is therefore limited to constant velocity rotation about $y$-axis of inertial frame $\{F\}$.

The environment is modelled with two rigid bodies: a cube to be observed as the target object, and a stationary rectangular prism enclosing the sensor and cube acting as the background. The various cube motions that will be simulated to test the observer's performance can be classed as either
\begin{enumerate}
\item ${\textbf{W}_c} = \textbf{0}$
\item ${\textbf{W}_c} \neq \textbf{0}$
\end{enumerate}

For case 1. the wrench and screw are constant so only the initial value is required. It is more efficient to represent the pose of a rigid body with just position and orientation in this case. The pose can be quickly computed by interpolating between an initial and final pose. For case 2. the screw, twist and wrench must be integrated numerically.

\subsubsection{Interpolation}
To compute a trajectory of $k$ poses beginning at $\{\mathbf{p}_i,\mathbf{q}_i\}$ and ending at $\{\mathbf{p}_f,\mathbf{q}_f\}$:\\

Poses for each time $\mathbf{t} =
					\begin{bmatrix}
						t_1 & t_2 & t_3 & \dots & t_k
					\end{bmatrix}$

Linear interpolation for position $\mathbf{P}= \begin{bmatrix}
													\mathbf{p}_1 & \mathbf{p}_2 & \mathbf{p}_3 & \dots & \mathbf{p}_k
											   \end{bmatrix}$:
\begin{equation}
	\mathbf{P} = 
	{\mathbf{p}_1}_{[1 \times k]} + (\mathbf{p}_k - \mathbf{p}_1)\frac{\mathbf{t}-{\mathbf{t}_1}_{[1 \times k]}}{t_k - t_1}
\end{equation}
OR?
\begin{equation}
	\mathbf{P} = 
	{\mathbf{P}_1}_{[1 \times k]} + (\mathbf{p}_k - \mathbf{p}_1)\frac{\mathbf{t}-{\mathbf{t}_1}_{[1 \times k]}}{t_k - t_1}
\end{equation}


Spherical linear interpolation for orientation quaternion $\mathbf{q}= \begin{bmatrix}
													\mathbf{q}_1 & \mathbf{q}_2 & \mathbf{q}_3 & \dots & \mathbf{q}_k
												\end{bmatrix}$:

\begin{equation} \label{slerp}
	\mathbf{Q} = \frac{\mathbf{q}_1\sin((\mathbf{1}_{[1 \times k]}-\mathbf{t})\theta) + \mathbf{q}_k\sin(\mathbf{t}\theta)}{\sin(\theta)}
\end{equation}
where
\begin{equation}
	\theta = \cos^{-1}(\mathbf{q}_1 \cdot \mathbf{q}_k)
\end{equation}

*MAKE THESE CLEARER

SCANNING:\\
Require multiple views of target object,reverse trajectory, concatenate and replicate\\
concatenate:\\
\begin{equation}
	\mathbf{P}_{loop}= 
	\begin{bmatrix}
		\mathbf{p}_1 & \mathbf{p}_2 & \mathbf{p}_3 & \dots & \mathbf{p}_k &
		\mathbf{p}_{k} & \mathbf{p}_{k-1} & \mathbf{p}_{k-2} & \dots & \mathbf{p}_1
	\end{bmatrix}
\end{equation}
\begin{equation}
	\mathbf{Q}_{loop}= 
	\begin{bmatrix}
		\mathbf{q}_1 & \mathbf{q}_2 & \mathbf{q}_3 & \dots & \mathbf{q}_k &
		\mathbf{q}_{k} & \mathbf{q}_{k-1} & \mathbf{q}_{k-2} & \dots & \mathbf{q}_1
	\end{bmatrix}
\end{equation}
replicate:
\begin{equation}
	\mathbf{P} = {\mathbf{P}_{loop}}_{[1 \times k]}
\end{equation}
\begin{equation}
	\mathbf{Q} = {\mathbf{Q}_{loop}}_{[1 \times k]}
\end{equation}



\subsubsection{Numerical Integration}
The time evolution of the screw, twist and wrench is computed iteratively from initial conditions by numerically integrating the ODEs in section \ref{kinematics}. For a rigid body with an associated reference frame $\{X\}$:

\begin{equation}
	\mathbf{S}_X(t+\delta t) = \mathbf{S}_X(t)\exp({\delta t {\mathbf{T}_X(t)}})
\end{equation}

\begin{equation}
	\mathbf{T}_X(t+\delta t) = \mathbf{T}_X(t) + \delta t \mathbf{W}_X(t)
\end{equation}

Assuming constant acceleration
\begin{equation}
	\mathbf{W}_X(t+\delta t) =\mathbf{W}_X(t)
\end{equation}

*will add runge-kutta method to simulation. allow choice of numerical method in config.\\
*add equations for RK4 
*not essential - ground truth is ground truth, experimental data won't be nearly as smooth anyway

\subsection{Sensor modelling}
The state of the sensor $\mathbf{X}_{s}(t)$ consists of terms corresponding to its motion and scanning operation. 

\textbf{motion:}\\
Since motion restricted, state of sensor actually implemented as 
\begin{equation}
	\mathbf{X}_{s}(t) = \{\mathbf{p}_s(t),\mathbf{q}_s(t),{^{A}\mathbf{n}(t)}\}
\end{equation}

Stationary position:
\begin{equation}
	\mathbf{p}_1 = \mathbf{p}_2 = \mathbf{p}_3 = \dots =  \mathbf{p}_k = 
	\begin{bmatrix}
		0 \\ 0 \\ 0
   	\end{bmatrix}
\end{equation}

													
Rotating from $-\phi$ to $\phi$ about $y$-axis of inertial frame ie interpolate between $\mathbf{q}_1$ and $\mathbf{q}_k$ with equation \ref{slerp}.
	
\begin{equation}
	\mathbf{q}_1 = \begin{bmatrix}
				 	\cos(-\phi/2) \\
				 	\sin(-\phi/2){\begin{bmatrix}
								 	0 \\ 1 \\ 0
							   	 \end{bmatrix}}
				 \end{bmatrix}
				 = \begin{bmatrix}
		 		   		\cos(\phi/2) \\ 0 \\ -\sin(\phi/2) \\ 0
				   \end{bmatrix}
\end{equation}

\begin{equation}
	\mathbf{q}_k = \begin{bmatrix}
				 	\cos(\phi/2) \\
				 	\sin(\phi/2){\begin{bmatrix}
								 	0 \\ 1 \\ 0
							   	 \end{bmatrix}}
				 \end{bmatrix}
				 = \begin{bmatrix}
		 		   		\cos(\phi/2) \\ 0 \\ \sin(\phi/2) \\ 0
				   \end{bmatrix}
\end{equation}

\textbf{scanning:}\\
Scanning behaviour determines ${^{A}\mathbf{n}(t)}$.

sensor parameters:
\begin{itemize}
\item field of view ($-\theta$ to $\theta$, about -z-axis of sensor frame ie anticlockwise about z-axis)
\item angular resolution $d\theta$ (no. steps in 1 rev - 1024, $d\theta = 2\pi/1024$)
\item time per revolution (1/24s = 24Hz)
\item measurement resolution (range measurements to closest mm)
\item measurement range (from 10cm to 4m)
\end{itemize}
From these parameters, create vector $\mathbf{n}(t)$. At each time, n is either a unit vector in direction of scan, in body fixed frame, or returns no value for when sensor is not returning a measurement (outside FOV)

scan direction: $^{A}\mathbf{n}(t)$
\begin{equation}
^{A}\mathbf{n}(t) =
	\begin{cases} 
	      \hfill \begin{bmatrix}
	      		\cos(-\theta + 2\pi t') \\
	      		-\sin(-\theta + 2\pi t') \\
	      		0
	      	\end{bmatrix}    \hfill & \text{ if $t' \leq \theta/pi$, $t' = k\delta\tau$ where $k \in \mathbb{N}$} \\
	      \hfill \mathbf{0} \hfill & \text{ if $t' > \theta/pi$, $t' \neq k\delta\tau$ where $k \in \mathbb{N}$} \\
	\end{cases} 
\end{equation}
where
\begin{equation}
t' = \mod(t,1/d\theta)\:d\theta
\end{equation}

*$\theta_0$ is start of FOV, $t'=X$ at end of FOV

Scan direction in inertial frame required for measurement simulation:
\begin{equation}
	{^{F}\mathbf{n'}(t)} = \mathbf{X}_s(t)\:{^{A}\mathbf{n'}(t)}
\end{equation}

\subsection{Environment Modelling}

\textbf{motion:}\\
Each object modelled with S. from initial conditions, numerically integrate S,T,W. If constant velocity motion, faster to use waypoints like sensor.
From pose of object at each time, compute pose of all points that make up object.

\textbf{rigid objects:}\\
Environment composed of rectangular prisms. These objects are modelled as an ordered set of 8 points in the inertial reference frame, and 12 triangles. Each triangle is a set of 3 integers, indicating the index of the three points that make up its vertexes.
The position of each of these points at each time is determined using the screw matrix and side lengths of the object.

Figure ~\ref{fig:cubeproblem}
\input{Figures/fig_triangles}

points in body frame:
initialPoints = s*0.5*[];
\begin{equation}
	P = \frac{1}{2}s
	\begin{bmatrix}
		-1  &  -1  &  -1  &  -1  &   1  &   1  &   1  &  1 \\
		-1  &  -1  &   1  &   1  &  -1  &  -1  &   1  &  1 \\
		-1  &   1  &  -1  &   1  &  -1  &   1  &  -1  &  1 
	\end{bmatrix}
\end{equation}
triangles:
\begin{equation}
	Tri = 
	\begin{bmatrix}
	1 & 2 & 3 \\
	2 & 4 & 3 \\
    4 & 3 & 7 \\
    4 & 8 & 7 \\
    5 & 6 & 7 \\
    8 & 6 & 7 \\
    2 & 6 & 5 \\
    2 & 1 & 5 \\
    2 & 6 & 8 \\
    2 & 4 & 8 \\
    1 & 5 & 7 \\
    1 & 3 & 7
	\end{bmatrix}
\end{equation}

DIAGRAM HERE

\subsection{Measurement Modelling}
	\subsubsection{Range Computation}
	Given the screw matrix (in the inertial frame) and scan direction (in the body fixed frame) of the sensor, the position and scan direction in the inertial frame are determined.
	The distance to the nearest environment object from this point, along the scan direction is determined with the M{\"o}ller-Trumbore ray-triangle intersection algorithm.

	\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{(O,D,pts,tri), where O and D are origin and direction of ray, pts are points in $\mathbb{R}^3$, tri are indexes of points making up vertexes of triangles}
	\KwOut{(intersect, range, angle, closest), where intersect is binary - collision or none. range is distance to intersection point, angle is incidence angle with surface hit, closest in index of triangle hit}	
	
	-initialise outputs\\
	-triangle vertexes\\
	-triangle edges\\
	-determinant\\
	-barycentric coords\\
	-intersection vector\\
	-if any intersection - minimum range\\
		-min range, angle, triangle ID
	
	\caption{M{\"o}ller-Trumbore ray-triangle intersection algorithm}
	\end{algorithm}

	\subsubsection{Noise Modelling}
	Noise model depends on sensor used.
	\begin{equation}
		\hat{r}(t) = f_s(r(t),\theta(t),\phi(k))
	\end{equation}
	where $\theta(t)$ is incidence angle of measurement, $\phi$ is surface properties of object $k$ that was measured, $f$ is some function for noise model of particular sensor.
	
	For Hokuyo UBG-04LX-F01 used, noise model was measured experimentally:
	\begin{equation}
		f_{UBG} = 
	\end{equation}
	
\subsection{Observer implementation}
	\subsubsection{Estimate: internal model}
		rigid body motion - numerical integration of ODEs
	
	\subsubsection{Identifying object/background}
		The variable \textit{observingObject} indicates whether the range measurement is of the target object or the background. It is assumed that initially the sensor will be observing background, so $\textit{observingObject}_0 = FALSE$

		\textbf{Range assumption:}
		Assuming that the object is within X, background outside
		PSEUDOCODE
		if range $\leq$ X
			observingObject = 1
		else
			observingObject = 0
		end
		
		\textbf{Continuity assumption:}
		if $abs(range_{i+1}-range_i) > X$
			observingObject = mod(observingObject+1,2)
		end
	\subsubsection{Update}
		\textbf{Orientation update:}\\
			at least 3 points, find planes and normal, rotate about cross of normals\\
			-> gives rotation axis
			
		\textbf{Position update:}\\
			find mean of intersection points of predicted, measured, vector subtraction
			-> translation vector
			
		\textbf{Size update:}\\
			if different pattern, delta s = dot(translation vector, scan direction)\\
			if same pattern, delta s = vector(mean measured points - mean predicted points)
			
		\textbf{Update scheme:}\\
			Can update S,T,W of state (or perhaps some combination)\\
			Scale rotation axis, translation vector, delta s depending on if update is via S,T,W
			
\section{Results}
plots for different initial conditions \& kinds of motion
	
