\chapter{Simulation}
\section{Implementation}
A simulation toolbox was implemented in Matlab to model scanning laser range-finder measurements and test observer schemes. The main components of the simulation are:
\begin{itemize}
\item rigid body trajectory computation;
\item solid object modelling;
\item range measurement simulation;
\item noise modelling;
\item observer implementation.
\end{itemize}

A high level description of the simulation is provided in Algorithm \ref{main}.\\
Load settings - saved data can be loaded here\\
Initialise sensor, computes pose of sensor, scan directions over time + creates sensor class instance with sensor settings for noise generation\\
Initialise environment, creates environment class instance - points in body frame, triangles, computes position of points over time\\
Initialise observer - creates observer class instance\\
For each time step, get state of sensor and environment \& compute range - this is done with a parallel for loop\\
Add noise to ranges\\
Observer simulation: each time step, update estimate based on estimated state (kinematics - numerical integration), predict range measurement, check if observing object, if yes update estimate  based on measured and predicted range.

\IncMargin{2em}	
	\begin{algorithm}[H]
	\DontPrintSemicolon
	\SetKwFunction{loadsettings}{loadsettings}
	\SetKwFunction{initialisesensor}{initialisesensor}
	\SetKwFunction{initialiseenvironment}{initialiseenvironment}
	\SetKwFunction{initialiseobserver}{initialiseobserver}
	\SetKwFunction{computerange}{computerange}
	\SetKwFunction{addnoise}{addnoise}
	\SetKwFunction{estimatestate}{estimatestate}
	\SetKwFunction{updatestate}{updatestate}
	\SetKwFunction{identifyobject}{identifyobject}	
	\KwData{\\
	%CHANGE THESE TO LETTERS
	$n_{steps}$ - no. steps in simulation\\
	$\mathbf{X}_s$ - pose and scan direction\\
	$\mathbf{X}_e$ - points for cube and background, triangles\\
	$\hat{\mathbf{X}}_c$ - estimate of pose and size of cube\\
	$c$ - true/false, current range measurement of cube\\
	$\mathbf{r}$ - ground truth range\\
	$\tilde{\mathbf{r}}$ - measured range - ground truth + noise\\
	$\hat{\mathbf{r}}$ - predicted range from state estimate\\
	$\bm{\alpha}$ - angle of incidence for each measurement\\
	$\mathbf{m}$ - index of triangle measured\\
	$\bm{\theta}$ - scan angle in sensor frame\\
	$\bm{\Theta}$ - set of scan angles that return range measurement\\
	}
	\Begin{
		$settings \leftarrow $\loadsettings\\
		$\mathbf{X}_s \leftarrow \initialisesensor(settings)$\\
		$\mathbf{X}_e  \leftarrow \initialiseenvironment(settings)$\\
		\initialiseobserver\\
		\For{$ii \leftarrow 1$ \KwTo $n_{steps}$}{
			\If{$\bm{\theta}[ii] \in \bm{\Theta}$}{
				$[\mathbf{r}[ii],\bm{\alpha}[ii],\mathbf{m}[ii]] \leftarrow \computerange(\mathbf{X}_s[ii],\mathbf{X}_e[ii])$
			}
		}
		$\tilde{\mathbf{r}} = \addnoise(\mathbf{r},\bm{\alpha},\mathbf{m},settings)$\\
		\For{$ii \leftarrow 1$ \KwTo $n_{steps}$}{
			$\hat{\mathbf{X}}_c[ii+1] \leftarrow \estimatestate(\hat{\mathbf{X}}_c[ii])$\\
			\If{$\bm{\theta}[ii] \in \bm{\Theta}$}{
				$\hat{\mathbf{r}}[ii] \leftarrow \computerange(\mathbf{X}_s[ii],\hat{\mathbf{X}}_c[ii])$
				$c \leftarrow \identifyobject(c,\tilde{\mathbf{r}})$\\
				\If{$c$}{
					$\hat{\mathbf{X}}_c[ii+1] \leftarrow \updatestate(\hat{\mathbf{X}}_c[ii+1])$
				}
			}
		}
	}
	\caption{Scanning range-finder and state observer simulation} \label{main}
	\end{algorithm}

\subsection{Rigid Body Motion}
To simulate range measurements the pose of the sensor and the objects comprising the environment must be computed at each time step. The computations required to do so can be reduced by taking into account the kinds of motion that must be simulated.

The observer actually computes the \textit{relative} position between the sensor and cube and simply uses knowledge of the sensor pose to determine the pose of the cube in the inertial frame. There is no need to simulate complex sensor motions because the motion of the cube can be adjusted to achieve the same result. The only requirement of the sensor motion is that a large field of view is acquired so that the entire target object can be viewed. The scanning behaviour of the sensor is to rotate back and forth about the $z$-axis of the body fixed frame $\{A\}$. To provide a rectangular field of view, the motion of the sensor is therefore limited to constant velocity rotation about $y$-axis of inertial frame $\{F\}$.

The environment is modelled with two rigid bodies: a cube to be observed as the target object, and a stationary rectangular prism enclosing the sensor and cube acting as the background. The various cube motions that will be simulated to test the observer's performance can be classed in terms of the wrench matrix of the cube as either
\begin{enumerate}
\item ${\textbf{W}_c} = \textbf{0}$
\item ${\textbf{W}_c} \neq \textbf{0}$
\end{enumerate}

For case 1. the wrench and screw are constant so only the initial value is required. It is more efficient to represent the pose of a rigid body with just position and orientation in this case. The pose can be quickly computed by interpolating between an initial and final pose. For case 2. the screw, twist and wrench must be integrated numerically.

\subsubsection{Interpolation}
To compute a trajectory of $k$ poses beginning at $\{\mathbf{p}_i,\mathbf{q}_i\}$ and ending at $\{\mathbf{p}_f,\mathbf{q}_f\}$:

Poses for each time $\mathbf{t} =
					\begin{bmatrix}
						t_1 & t_2 & t_3 & \dots & t_k
					\end{bmatrix}$

Linear interpolation for position $\mathbf{P}= \begin{bmatrix}
													\mathbf{p}_1 & \mathbf{p}_2 & \mathbf{p}_3 & \dots & \mathbf{p}_k
											   \end{bmatrix}$:
\begin{equation}
	\mathbf{P} = 
	{\mathbf{p}_1}_{[1 \times k]} + (\mathbf{p}_k - \mathbf{p}_1)\frac{\mathbf{t}-{\mathbf{t}_1}_{[1 \times k]}}{t_k - t_1}
\end{equation}
OR?
\begin{equation}
	\mathbf{P} = 
	{\mathbf{P}_1}_{[1 \times k]} + (\mathbf{p}_k - \mathbf{p}_1)\frac{\mathbf{t}-{\mathbf{t}_1}_{[1 \times k]}}{t_k - t_1}
\end{equation}


Spherical linear interpolation for orientation quaternion $\mathbf{q}= \begin{bmatrix}
													\mathbf{q}_1 & \mathbf{q}_2 & \mathbf{q}_3 & \dots & \mathbf{q}_k
												\end{bmatrix}$:

\begin{equation} \label{slerp}
	\mathbf{Q} = \frac{\mathbf{q}_1\sin((\mathbf{1}_{[1 \times k]}-\mathbf{t})\theta) + \mathbf{q}_k\sin(\mathbf{t}\theta)}{\sin(\theta)}
\end{equation}
where
\begin{equation}
	\theta = \cos^{-1}(\mathbf{q}_1 \cdot \mathbf{q}_k)
\end{equation}

*MAKE THESE CLEARER

SCANNING:\\
Require multiple views of target object,reverse trajectory, concatenate and replicate\\
concatenate:\\
\begin{equation}
	\mathbf{P}_{loop}= 
	\begin{bmatrix}
		\mathbf{p}_1 & \mathbf{p}_2 & \mathbf{p}_3 & \dots & \mathbf{p}_k &
		\mathbf{p}_{k} & \mathbf{p}_{k-1} & \mathbf{p}_{k-2} & \dots & \mathbf{p}_1
	\end{bmatrix}
\end{equation}
\begin{equation}
	\mathbf{Q}_{loop}= 
	\begin{bmatrix}
		\mathbf{q}_1 & \mathbf{q}_2 & \mathbf{q}_3 & \dots & \mathbf{q}_k &
		\mathbf{q}_{k} & \mathbf{q}_{k-1} & \mathbf{q}_{k-2} & \dots & \mathbf{q}_1
	\end{bmatrix}
\end{equation}
replicate:
\begin{equation}
	\mathbf{P} = {\mathbf{P}_{loop}}_{[1 \times k]}
\end{equation}
\begin{equation}
	\mathbf{Q} = {\mathbf{Q}_{loop}}_{[1 \times k]}
\end{equation}



\subsubsection{Numerical Integration} \label{integration}
The time evolution of the screw, twist and wrench is computed iteratively from initial conditions by numerically integrating the ODEs in section \ref{kinematics}. For a rigid body with an associated reference frame $\{X\}$:

\begin{equation}
	\mathbf{S}_X(t+\delta t) = \mathbf{S}_X(t)\exp({\delta t {\mathbf{T}_X(t)}})
\end{equation}

\begin{equation}
	\mathbf{T}_X(t+\delta t) = \mathbf{T}_X(t) + \delta t \mathbf{W}_X(t)
\end{equation}

Assuming constant acceleration
\begin{equation}
	\mathbf{W}_X(t+\delta t) =\mathbf{W}_X(t)
\end{equation}

*More accurate integration method not essential - ground truth is ground truth, experimental data won't be nearly as smooth anyway.

In practice, compute orientation quaternion from screw matrix - pose still represented with quaternion and vector - same as interpolation case.

\subsection{Sensor modelling}
The state of the sensor $\mathbf{X}_{s}(t)$ consists of terms corresponding to its motion and scanning operation. 

\subsubsection{Motion}
Since motion restricted, state of sensor actually implemented as 
\begin{equation}
	\mathbf{X}_{s}(t) = \{\mathbf{p}_s(t),\mathbf{q}_s(t),{^{A}\mathbf{s}(t)}\}
\end{equation}

Stationary position:
\begin{equation}
	\mathbf{p}_1 = \mathbf{p}_2 = \mathbf{p}_3 = \dots =  \mathbf{p}_k = 
	\begin{bmatrix}
		0 \\ 0 \\ 0
   	\end{bmatrix}
\end{equation}

													
Rotating from $-\phi$ to $\phi$ about $y$-axis of inertial frame ie interpolate between $\mathbf{q}_1$ and $\mathbf{q}_k$ with equation \ref{slerp}.
	
\begin{equation}
	\mathbf{q}_1 = \begin{bmatrix}
				 	\cos(-\phi/2) \\
				 	\sin(-\phi/2){\begin{bmatrix}
								 	0 \\ 1 \\ 0
							   	 \end{bmatrix}}
				 \end{bmatrix}
				 = \begin{bmatrix}
		 		   		\cos(\phi/2) \\ 0 \\ -\sin(\phi/2) \\ 0
				   \end{bmatrix}
\end{equation}

\begin{equation}
	\mathbf{q}_k = \begin{bmatrix}
				 	\cos(\phi/2) \\
				 	\sin(\phi/2){\begin{bmatrix}
								 	0 \\ 1 \\ 0
							   	 \end{bmatrix}}
				 \end{bmatrix}
				 = \begin{bmatrix}
		 		   		\cos(\phi/2) \\ 0 \\ \sin(\phi/2) \\ 0
				   \end{bmatrix}
\end{equation}

\subsubsection{Scanning}
The scanning behaviour of the sensor is modelled with the vector ${^{A}\mathbf{n}(t)}$.
To simulate a 2D scanning sensor the following parameters are used:
\begin{itemize}
\item field of view $[-\theta,\theta]$: rotation about -z-axis of sensor frame ie anticlockwise about z-axis. in practice, this is represented by start angle, scan direction and field of view angle
\item number of scans $n_{scans}$: number of scan angles in 1 rev. Gives angular resolution $d\theta = \dfrac{2\pi}{n_{scans}}$
\item revolutions per second $\Omega$: This gives each time step as $d\tau = \dfrac{1}{n_{scans}\Omega}$
\end{itemize}
From these parameters, create vector ${^{A}\mathbf{s}(t)}$. At each time $t$, ${^{A}\mathbf{s}(t)}$ is either a unit vector indicating the direction of measurement in the sensor frame, or has $\mathbf{0}$ magnitude the when sensor is not returning a measurement (outside FOV)

scan direction: $^{A}\mathbf{s}(t)$
\begin{equation}
^{A}\mathbf{n}(t) =
	\begin{cases} 
	      \hfill \begin{bmatrix}
	      		\cos(-\theta + 2\pi t') \\
	      		-\sin(-\theta + 2\pi t') \\
	      		0
	      	\end{bmatrix}    \hfill & \text{ if $t' \leq \theta/pi$, $t' = k\delta\tau$ where $k \in \mathbb{N}$} \\
	      \hfill \mathbf{0} \hfill & \text{ if $t' > \theta/pi$, $t' \neq k\delta\tau$ where $k \in \mathbb{N}$} \\
	\end{cases} 
\end{equation}
where
\begin{equation}
t' = \mod(t,1/d\theta)\:d\theta
\end{equation}

*$\theta_0$ is start of FOV, $t'=X$ at end of FOV

Scan direction in inertial frame required for measurement simulation:
\begin{equation}
	{^{F}\mathbf{d'}(t)} = \mathbf{X}_s(t)\:{^{A}\mathbf{d'}(t)}
\end{equation}

\subsection{Environment Modelling}
\subsubsection{Motion}
Pose of object is actually pose of centre of mass of object.
Each object modelled with S. from initial conditions, numerically integrate S,T,W. If constant velocity motion, faster to use waypoints like sensor.
From pose of object at each time, compute pose of all points that make up object: 3D array, each slice is set of points at time ii

\subsubsection{Rigid Objects}
Environment composed of rectangular prisms. These objects are modelled as an ordered set of 8 points in the inertial reference frame, and 12 triangles. Each triangle is a set of 3 integers, indicating the index of the three points that make up its vertexes.
The position (in the inertial frame) of each of these points at each time is determined using the screw matrix and position in body frame (side lengths of the object \& default cube points).

cube points in body frame:
initialPoints = s*0.5*[];
\begin{equation}
	\mathbf{P}_c = \frac{1}{2}s
	\begin{bmatrix*}[r]
		-1  &  -1  &  -1  &  -1  &   1  &   1  &   1  &  1 \\
		-1  &  -1  &   1  &   1  &  -1  &  -1  &   1  &  1 \\
		-1  &   1  &  -1  &   1  &  -1  &   1  &  -1  &  1 
	\end{bmatrix*}
\end{equation}
Shift each according to pose of sensor to get cube points in inertial frame. In sim, rotated with quaternions.
triangles:
\begin{equation}
	\mathbf{T} = 
	\begin{bmatrix}
	1 & 2 & 3 \\
	2 & 4 & 3 \\
    4 & 3 & 7 \\
    4 & 8 & 7 \\
    5 & 6 & 7 \\
    8 & 6 & 7 \\
    2 & 6 & 5 \\
    2 & 1 & 5 \\
    2 & 6 & 8 \\
    2 & 4 & 8 \\
    1 & 5 & 7 \\
    1 & 3 & 7
	\end{bmatrix}
\end{equation}

See figure~\ref{fig:cubeproblem}
\input{Figures/fig_triangles}

\subsection{Measurement Modelling}
	\subsubsection{Range Computation}
	Given the screw matrix (in the inertial frame) and scan direction (in the body fixed frame) of the sensor, the position and scan direction in the inertial frame are determined.
	The distance to the nearest environment object from this point, along the scan direction is determined with the M{\"o}ller-Trumbore ray-triangle intersection algorithm.
	Difficult to visualise from code, include diagram for case of ray \& single triangle?

	\IncMargin{2em}
	\begin{algorithm}
	\DontPrintSemicolon
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\SetKwFunction{size}{size}
	\SetKwFunction{any}{any}
	\SetKwFunction{min}{min}
	\SetKwFunction{find}{find}
	\SetKwFunction{atan}{atan}
	
	\Input{$\mathbf{o}$ - ray origin\\
		   $\mathbf{d}$ - ray direction vector\\
		   $\mathbf{P}$ - cube in inertial frame \\
		   $\mathbf{T}$ - triangle matrix}
	\Output{$x$ - True/False - measurement corresponds to object\\
			$r$ - distance to object in m\\
			$\theta$ - incidence angle in rad\\
			$m$ - index of triangle hit}
	\Begin{
		\tcc{initialise outputs}
		$x \longleftarrow 0$\\
		$r \longleftarrow NaN$\\
		$\theta \longleftarrow NaN$\\
		$m \longleftarrow NaN$\\
		\tcc{triangle vertexes and edges}
		$\mathbf{V}_1 \longleftarrow \mathbf{P}[\mathbf{T}[:,1]]$\\
		$\mathbf{V}_2 \longleftarrow \mathbf{P}[\mathbf{T}[:,2]]$\\
		$\mathbf{V}_3 \longleftarrow \mathbf{P}[\mathbf{T}[:,3]]$\\
		$\mathbf{E}_1 \longleftarrow \mathbf{V}_2 - \mathbf{V}_1$\\
		$\mathbf{E}_2 \longleftarrow \mathbf{V}_3 - \mathbf{V}_1$\\
		$m = \size(\mathbf{V}_1,1)$\\
		$\mathbf{A} \longleftarrow \mathbf{o}_{[m \times 1]} - \mathbf{P}$\\
		\tcc{determinant}
		$\mathbf{B} \longleftarrow \mathbf{d}_{[m \times 1]} \times \mathbf{E}_2$ *(along dim 2)\\
		$\bm{\delta} \longleftarrow \mathbf{E}_1 \cdot \mathbf{B}$ *(along dim 2)\\		
		$\mathbf{y} \longleftarrow |\bm{\delta}|\leq\mathbf{0}$\\
		$\bm{\delta}[\mathbf{y}] \longleftarrow \mathbf{NaN}$\\
		\tcc{barycentric coordinates}
		$\mathbf{u} \longleftarrow (\mathbf{A} \cdot \mathbf{B})/\bm{\delta}$ *(along dim 2)\\
		$\mathbf{Q} \longleftarrow \mathbf{A} \times \mathbf{E}_1$ *(along dim 2)\\
		$\mathbf{v} \longleftarrow (\mathbf{d}_{[n \times 1]} \cdot \mathbf{Q})/\bm{\delta}$ *(along dim 2)\\
		$\mathbf{s} \longleftarrow (\mathbf{E}_2 \cdot \mathbf{Q})/\bm{\delta}$\\
		\tcc{intersection vector}	
		$\mathbf{z} \longleftarrow \mathbf{y} \textbf{ and } (\mathbf{u} \geq \mathbf{0}) 
		\textbf{ and } (\mathbf{v} \geq \mathbf{0}) \textbf{ and } (\mathbf{u}+\mathbf{v} \leq 
		\mathbf{0})$\\
		$\mathbf{x} \longleftarrow \mathbf{z} \textbf{ and } (\mathbf{s} \geq \mathbf{0})$\\
		\If{$\any(\mathbf{x})$}{
			$x \longleftarrow 1$\\
			$\mathbf{x}[\textbf{not }\mathbf{x}] \longleftarrow \mathbf{NaN}$\\
			$\mathbf{r} = \mathbf{s} \circ \mathbf{x}$\\
			$r = \min(\mathbf{r})$\\
			$m = \find(\mathbf{r} = r,1)$\\
			$\mathbf{e}_1 \longleftarrow \mathbf{E}_1[t,:]$\\ 
			$\mathbf{e}_2 \longleftarrow \mathbf{E}_2[t,:]$\\ 
			$\mathbf{n} = \mathbf{e}_1 \times \mathbf{e}_2$\\
			$\theta = \atan(|\mathbf{d}\times\mathbf{n}|,\mathbf{d}\cdot\mathbf{n})$ *atan2\\
			$\theta = \min(\theta,\pi-\theta)$
		}
	}
	\caption{M{\"o}ller-Trumbore ray-triangle intersection\label{MTalg}}
	\end{algorithm}
	
	\subsubsection{Object Surface}
	random walk
	
	\subsubsection{Sensor Noise}
	Noise model depends on sensor used.
	\begin{equation}
		\hat{r}(t) = f_s(r(t),\theta(t),\phi(k))
	\end{equation}
	where $\theta(t)$ is incidence angle of measurement, $\phi$ is surface properties of object $k$ that was measured, $f$ is some function for noise model of particular sensor.
	
	For Hokuyo UBG-04LX-F01 used, noise model was measured experimentally:
	\begin{equation}
		f_{UBG}(r,\theta,\phi) = 
	\end{equation}
	
	
\subsection{Observer implementation}
	\subsubsection{Estimate: internal model}
		The state of the cube at each time step is estimated using the numerical integration method described in section \ref{integration}.
	
	\subsubsection{Identifying object/background}
		The variable \textit{observingObject} indicates whether the range measurement is of the target object or the background. It is assumed that initially the sensor will be observing background, so $\textit{observingObject}_0 = FALSE$

		\textbf{Range assumption:}
		Assuming that the object is within X, background outside
		PSEUDOCODE
		if range $\leq$ X
			c = 1
		else
			c = 0
		end
		
		\textbf{Difference assumption:}
		if $abs(range_{i+1}-range_i) > X$
			c = mod(c+1,2)
		end
		
		\IncMargin{2em}
		\begin{algorithm}
		\DontPrintSemicolon
		\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	
		\Input{$rangeAssumption$ - true/false\\
			   $differenceAssumption$ - true/false\\
			   $r_{max}$ - max range for cube\\
			   $\Delta_{max}$ - max diff between measurements of same object\\
			   $c$ - true/false - current measurement is of cube\\
			   $\mathbf{r}_{i+1}$ - distance to object at $t = i+1$\\
			   $\mathbf{r}_{i}$ - distance to object at $t = i$}
		\Output{$c$ - true/false}
		\Begin{				
			\If{$differenceAssumption$}{
				\If{$|\mathbf{r}_{i+1}-\mathbf{r}_{i}| > \Delta_{max}$}{
					$c = \mod(c + 1,2)$
				}
			}
			\If{$rangeAssumption$}{
				\If{$\mathbf{r}_{i+1} > r_{max}$}{
					$c = 0$
				}
			}
		}
		\caption{Target/background object separation}
		\end{algorithm}
		
	\subsubsection{Update}
		\textbf{Input ranges:}\\
			Use ranges according to ordered sequence of indexes $a$. ie if all present - quadrilateral with points at corners:
			\begin{equation}
				a = \{ii,(ii-1),(ii-n_{scans}),(ii-1-n_{scans})\}
			\end{equation}
		
		\textbf{Orientation update:}\\
			Require at least 3 ranges in prediction and measurement. ie
			$|\hat{a}| \geq 3$ and $|\tilde{a}| \geq 3$
	
			From range and direction, compute coordinates of intersection\\
			\begin{equation}
				\mathbf{P}(a_k) = \mathbf{R}(a_k){^{F}\mathbf{N}(a_k)}
			\end{equation}
			\begin{equation}
				\mathbf{n} = [\mathbf{P}(a_2) - \mathbf{P}(a_1)] \times [\mathbf{p}(a_3) - \mathbf{p}(a_1)]
			\end{equation}
			Rotation axis from cross product of predicted and measured normals:\\
			\begin{equation}
				\mathbf{r}_{update} = \hat{\mathbf{n}} \times \tilde{\mathbf{n}}
			\end{equation}
			Rotation angle determined in settings. Table REF shows settings	
			
			figure~\ref{fig:orientation}
			\input{Figures/fig_orientationupdate}
			
		\textbf{Position update:}\\
			Require at least 1 range in prediction and measurement:\\
			$|\hat{a}| \geq 1$ \and $|\tilde{a}| \geq 1$\\
			Average to get $\hat{\bm{\mu}}_\mathbf{p}$ and $\tilde{\bm{\mu}}_\mathbf{p}$\\
			Scale translation vector - taking into account geometry of intersection points and sensor position.
			Get mean of all predicted and measured ranges = $\mu_{r}$. Need valid scan direction ie within FOV for $ii,ii-1,ii-1-n_{scans}$
			$\mathbf{p}_0 = \mathbf{p}_s(t) = {^{F}_{F}\mathbf{p}^{}_{A}(t)}$\\
			$\mathbf{p}_1 = \mu_{r}{^{F}\mathbf{s}(ii)}$\\	
			$\mathbf{p}_2 = \mu_{r}{^{F}\mathbf{s}(ii-1)}$\\
			$\mathbf{p}_3 = \mu_{r}{^{F}\mathbf{s}(ii-1-n_{scans})}$\\
			\begin{equation}
				\mathbf{p}_{update} = 
				\begin{bmatrix}
					\frac{1}{|\mathbf{p}_1-\mathbf{p}_0|} & 0 & 0\\
					0 & \frac{1}{|\mathbf{p}_2-\mathbf{p}_1|} & 0\\
					0 & 0 & \frac{1}{|\mathbf{p}_3-\mathbf{p}_2|}
				\end{bmatrix}
				(\tilde{\bm{\mu}}_\mathbf{p}-\hat{\bm{\mu}}_\mathbf{p})
			\end{equation}
			
			figure~\ref{fig:position}
			\input{Figures/fig_positionupdate}
			
		\textbf{Size update:}\\
			If different pattern $\hat{a} \neq \tilde{a}$\\
			At least 1 of each: $|\hat{a}| \geq 1$ \and $|\tilde{a}| \geq 1$\\
			\begin{equation}
				s_{update} = \mathbf{p}_{update} \cdot {^{F}\mathbf{d}(ii)}
			\end{equation}
			figure~\ref{fig:size1}
			\input{Figures/fig_sizeupdate1}
			
			If same pattern $\hat{a} = \tilde{a}$\\
			At least 1 of each: $|\hat{a}| \geq 1$ \and $|\tilde{a}| \geq 1$\\
			delta s = mean measured ranges - mean predicted ranges\\
			\begin{equation}
				s_{update} = \tilde{\mu}_r - \hat{\mu}_r
			\end{equation}	
				
			figure~\ref{fig:size2}
			\input{Figures/fig_sizeupdate2}
			
		\textbf{Update scheme:}\\
					
			Can update S,T,W of state (or perhaps some combination)\\
			Scale rotation axis, translation vector, delta s depending on if update is via S,T,W\\
			S: r to R, rotate orientation. Shift origin position with translation vector.\\
			T: add r to angular velocity. Add v to linear velocity.\\
			W: add r to angular acceleration. Add v to linear acceleration.

			TABLE - weights from config.
			\input{Tables/tab_updateweights}
			
\section{Results}
plots for different initial conditions \& kinds of motion
	
