\chapter{Simulation}
\section{Implementation}
A simulation toolbox was implemented in Matlab to model scanning laser range-finder measurements and test observer schemes. The main components of the simulation are:
\begin{itemize}
\item rigid body trajectory computation;
\item solid object modelling;
\item range measurement simulation;
\item noise modelling;
\item observer implementation.
\end{itemize}

\IncMargin{2em}	
	\begin{algorithm}
	\DontPrintSemicolon
	\SetKwFunction{loadsettings}{loadsettings}
	\SetKwFunction{initialisesensor}{initialisesensor}
	\SetKwFunction{initialiseenvironment}{initialiseenvironment}
	\SetKwFunction{initialiseobserver}{initialiseobserver}
	\SetKwFunction{computerange}{computerange}
	\SetKwFunction{addnoise}{addnoise}
	\SetKwFunction{estimatestate}{estimatestate}
	\SetKwFunction{updatestate}{updatestate}
	\SetKwFunction{identifyobject}{identifyobject}	
	\KwData{\\
	%CHANGE THESE TO LETTERS
	$n_{steps}$ - no. steps in simulation\\
	$\mathbf{X}_s$ - pose and scan direction\\
	$\mathbf{X}_e$ - points for cube and background, triangles\\
	$\hat{\mathbf{X}}_c$ - estimate of pose and size of cube\\
	$c$ - true/false, current range measurement is of cube\\
	$\mathbf{r}$ - ground truth range\\
	$\tilde{\mathbf{r}}$ - measured range - ground truth + noise\\
	$\hat{\mathbf{r}}$ - predicted range from state estimate\\
	$\bm{\alpha}$ - angle of incidence for each measurement\\
	$\mathbf{m}$ - index of triangle measured\\
	$\bm{\theta}$ - scan angle in sensor frame\\
	$\bm{\Theta}$ - set of scan angles that return range measurement\\
	}
	\Begin{
		$settings \leftarrow $\loadsettings\\
		$\mathbf{X}_s \leftarrow \initialisesensor(settings)$\\
		$\mathbf{X}_e  \leftarrow \initialiseenvironment(settings)$\\
		\initialiseobserver\\
		\For{$ii \leftarrow 1$ \KwTo $n_{steps}$}{ \label{rangesloop}
			\If{$\bm{\theta}[ii] \in \bm{\Theta}$}{ \label{FOV}
				$[\mathbf{r}[ii],\bm{\alpha}[ii],\mathbf{m}[ii]] \leftarrow \computerange(\mathbf{X}_s[ii],\mathbf{X}_e[ii])$
			}
		}
		$\tilde{\mathbf{r}} = \addnoise(\mathbf{r},\bm{\alpha},\mathbf{m},settings)$ \label{addnoise}\\
		\For{$ii \leftarrow 1$ \KwTo $n_{steps}$}{ \label{observerloop}
			$\hat{\mathbf{X}}_c[ii+1] \leftarrow \estimatestate(\hat{\mathbf{X}}_c[ii])$\\
			\If{$\bm{\theta}[ii] \in \bm{\Theta}$}{
				$\hat{\mathbf{r}}[ii] \leftarrow \computerange(\mathbf{X}_s[ii],\hat{\mathbf{X}}_c[ii])$\\
				$c \leftarrow \identifyobject(c,\tilde{\mathbf{r}})$ \label{object}\\
				\If{$c$}{
					$\hat{\mathbf{X}}_c[ii+1] \leftarrow \updatestate(\hat{\mathbf{X}}_c[ii+1],\mathbf{X}_s[ii],\hat{\mathbf{r}},\tilde{\mathbf{r}})$
				}
			}
		}
	}
	\caption{Scanning range-finder and state observer simulation} \label{main}
	\end{algorithm}
	
A high level description of the simulation is provided in Algorithm \ref{main}.	
First, a settings file is loaded. The most important settings determined here are the trajectories of the sensor and environment objects, the scanning behaviour of the sensor and the observer update function.
Next, the sensor class instance is initialised with \texttt{initialisesensor}. This requires computation of the pose and scanning directions of the sensor over time. Similarly, initialisation of the environment through \texttt{initialiseenvironment} requires computation of the pose of each rigid body comprising it. The surfaces of the bodies are then represented with a set of points and corresponding triangles. The position of each point with respect to the inertial frame $\{F\}$ is computed at each time step. The settings file provides the initial conditions with which the observer is initialised in \texttt{initialiseobserver}. 
Beginning on line \ref{rangesloop}, the state of the sensor and environment are used to compute the ground truth range measurements $\mathbf{r}$ at each time step. The incidence angle of the between the scan direction and object, as well as the index of the triangle hit are also stored as they will be required for sensor noise modelling. This is performed with a parallel \texttt{for} loop to speed up computation. Line \ref{FOV} ensures ranges are only computed when the current scan direction is within the sensor's field of view.
In line \ref{addnoise}, noise is simulated and added to the ground truth ranges to produce the measured ranges $\tilde{\mathbf{r}}$.
The \texttt{for} loop beginning on line \ref{observerloop} begins the observer simulation. 
At each time step, \texttt{estimatestate} estimates the state of the cube $\hat{\mathbf{X}}_c$ from the previous state with the kinematics model in \ref{kinematics}.
From the sensor state $\mathbf{X}_s$ and the estimated state of the cube $\hat{\mathbf{X}}_c$, \texttt{computerange} is used to determined the predicted range measurement $\hat{\mathbf{r}}$.
The variable $c$ indicates whether the current range measurement corresponds to the cube or the background. On line \ref{object} the measured ranges and previous value of $c$ are used to determine whether the current measurement corresponds to the cube. If it does, the cube state estimate $\hat{\mathbf{X}}_c$ is updated using the previous state estimate, current sensor state, and the predicted and measured ranges.


\subsection{Rigid Body Motion} \label{motion}
To simulate range measurements the pose of the sensor and the objects comprising the environment must be computed at each time step. The computations required to do so can be reduced by taking into account the kinds of motion that must be simulated.

The observer actually computes the \textit{relative} position between the sensor and cube and simply uses knowledge of the sensor pose to determine the pose of the cube in the inertial frame. There is no need to simulate complex sensor motions because the motion of the cube can be adjusted to achieve the same result. The only requirement of the sensor motion is that a large field of view is acquired so that the entire target object can be viewed. The scanning behaviour of the sensor is to rotate back and forth about the $z$-axis of the body fixed frame $\{A\}$. To provide a rectangular field of view, the motion of the sensor is therefore limited to constant velocity rotation about $y$-axis of inertial frame $\{F\}$.

The environment is modelled with two rigid bodies: a cube to be observed as the target object, and a stationary rectangular prism enclosing the sensor and cube acting as the background. The various cube motions that will be simulated to test the observer's performance can be classed in terms of the wrench matrix of the cube as either
\begin{enumerate}
\item ${\textbf{W}_c} = \textbf{0}$
\item ${\textbf{W}_c} \neq \textbf{0}$
\end{enumerate}

For case 1. the wrench and screw are constant so only the initial value is required. It is more efficient to represent the pose of a rigid body with just position and orientation in this case. The pose can be quickly computed by interpolating between an initial and final pose. For case 2. the screw, twist and wrench must be integrated numerically.

\subsubsection{Interpolation}
For the case of zero wrench, the pose of the body can be represented with a position vector and orientation quaternion. A trajectory of $k$ poses at times 
$\mathbf{t} =
\begin{bmatrix}
	t_1 & t_2 & t_3 & \dots & t_k
\end{bmatrix}$,
is computed by interpolating from $\{\mathbf{p}_i,\mathbf{q}_i\}$ to $\{\mathbf{p}_f,\mathbf{q}_f\}$.

The position vectors
$\mathbf{P}= \begin{bmatrix}
\mathbf{p}_1 & \mathbf{p}_2 & \mathbf{p}_3 & \dots & \mathbf{p}_k
\end{bmatrix}$
 are computed with:
\begin{equation}
	\mathbf{P} = 
	{\mathbf{p}_1}_{[1 \times k]} + (\mathbf{p}_k - \mathbf{p}_1)\frac{\mathbf{t}-{\mathbf{t}_1}_{[1 \times k]}}{t_k - t_1}
\end{equation}

Spherical linear interpolation is used to compute the orientation
$\mathbf{Q}= \begin{bmatrix}
	\mathbf{q}_1 & \mathbf{q}_2 & \mathbf{q}_3 & \dots & \mathbf{q}_k
\end{bmatrix}$
at each time:
\begin{equation} \label{slerp}
	\mathbf{Q} = \frac{\mathbf{q}_1\sin((\mathbf{1}_{[1 \times k]}-\mathbf{t})\theta) + \mathbf{q}_k\sin(\mathbf{t}\theta)}{\sin(\theta)}
\end{equation}
where
\begin{equation}
	\theta = \cos^{-1}(\mathbf{q}_1 \cdot \mathbf{q}_k)
\end{equation}

This interpolation method is used to compute the trajectory of the sensor. To acquire multiple views of the  entire cube, the sensor must pan back and forth several times. This is achieved by first reversing the trajectory and concatenating with the original to produce the looped trajectories $\mathbf{P}_{loop}$ and $\mathbf{Q}_{loop}$:
\begin{equation}
	\mathbf{P}_{loop}= 
	\begin{bmatrix}
		\mathbf{p}_1 & \mathbf{p}_2 & \mathbf{p}_3 & \dots & \mathbf{p}_k &
		\mathbf{p}_{k} & \mathbf{p}_{k-1} & \mathbf{p}_{k-2} & \dots & \mathbf{p}_1
	\end{bmatrix}
\end{equation}
\begin{equation}
	\mathbf{Q}_{loop}= 
	\begin{bmatrix}
		\mathbf{q}_1 & \mathbf{q}_2 & \mathbf{q}_3 & \dots & \mathbf{q}_k &
		\mathbf{q}_{k} & \mathbf{q}_{k-1} & \mathbf{q}_{k-2} & \dots & \mathbf{q}_1
	\end{bmatrix}
\end{equation}

This looped trajectory is repeated $k$ times acquire multiple views:
\begin{equation}
	\mathbf{P} = {\mathbf{P}_{loop}}_{[1 \times k]}
\end{equation}
\begin{equation}
	\mathbf{Q} = {\mathbf{Q}_{loop}}_{[1 \times k]}
\end{equation}


\subsubsection{Numerical Integration} \label{integration}
The time evolution of the screw, twist and wrench is computed iteratively from initial conditions by numerically integrating the ODEs in section \ref{kinematics}. For a rigid body with an associated reference frame $\{X\}$, moving with constant acceleration:

\begin{equation}
	\mathbf{S}_X(t+\delta t) = \mathbf{S}_X(t)\exp({\delta t {\mathbf{T}_X(t)}})
\end{equation}

\begin{equation}
	\mathbf{T}_X(t+\delta t) = \mathbf{T}_X(t) + \delta t \mathbf{W}_X(t)
\end{equation}

\begin{equation}
	\mathbf{W}_X(t+\delta t) =\mathbf{W}_X(t)
\end{equation}

Though a higher order integration method, such as Runge-Kutta could be used to compute a trajectory that more accurately represents a constant acceleration, this is not strictly necessary. The observer performance is unlikely to be affected by how constant the acceleration is. Furthermore, it is likely that the experimentally collected data will have even larger variations in acceleration.

To simplify the code, the position vector and orientation quaternion are computed from the screw matrix. This allows the same functions to be used in either the interpolation or numerical integration cases. Multiplying the points that make up the rigid objects can also be done more compactly with quaternions.

\subsection{Sensor modelling}
\subsubsection{Motion}
The state of the sensor $\mathbf{X}_{s}(t)$ consists of terms corresponding to its motion and scanning operation. Since the motion of the sensor is restricted to zero acceleration, the state of sensor can be computed with the interpolation method and represented with position, orientation and scanning direction. 
\begin{equation}
	\mathbf{X}_{s}(t) = \{\mathbf{p}_s(t),\mathbf{q}_s(t),{^{A}\mathbf{d}(t)}\}
\end{equation}

Since it has a stationary position, the position of the sensor over time is fixed at the origin of the inertial frame $\{F\}$.
\begin{equation}
	\mathbf{p}_1 = \mathbf{p}_2 = \mathbf{p}_3 = \dots =  \mathbf{p}_k = 
	\begin{bmatrix}
		0 \\ 0 \\ 0
   	\end{bmatrix}
\end{equation}
										
The sensor rotates between $-\phi$ and $\phi$ about the $y$-axis of inertial frame $\{F\}$. Thus, its orientation is computed by interpolating between $\mathbf{q}_1$ and $\mathbf{q}_k$ with equation \ref{slerp}.
	
\begin{equation}
	\mathbf{q}_1 = \begin{bmatrix}
				 	\cos(-\phi/2) \\
				 	\sin(-\phi/2){\begin{bmatrix}
								 	0 \\ 1 \\ 0
							   	 \end{bmatrix}}
				 \end{bmatrix}
				 = \begin{bmatrix}
		 		   		\cos(\phi/2) \\ 0 \\ -\sin(\phi/2) \\ 0
				   \end{bmatrix}
\end{equation}

\begin{equation}
	\mathbf{q}_k = \begin{bmatrix}
				 	\cos(\phi/2) \\
				 	\sin(\phi/2){\begin{bmatrix}
								 	0 \\ 1 \\ 0
							   	 \end{bmatrix}}
				 \end{bmatrix}
				 = \begin{bmatrix}
		 		   		\cos(\phi/2) \\ 0 \\ \sin(\phi/2) \\ 0
				   \end{bmatrix}
\end{equation}

\subsubsection{Scanning}
The scanning behaviour of the sensor is modelled with the vector ${^{A}\mathbf{d}(t)}$.
To simulate a 2D scanning sensor the following parameters are used:
\begin{itemize}
\item field of view $\mathbf{\Theta}$: The vector ${^{A}\mathbf{d}(t)}$ rotates anti-clockwise about the $z$ axis of the sensor frame $\{A\}$. Measurements are only taken when the scan angle about is between $-\theta$ and $\theta$ about the $-z$-axis of the sensor frame $\{A\}$. In practice, the field of view is implemented as the start angle $-\theta$, direction of rotation and angular range $2\theta$.
\item number of scans $n_{scans}$: This represents the number of scan angles in a single revolution. Since measurements are limited by the field of view of the sensor, the actual number of measurements per second is $n_{ranges} = \frac{2\theta}{2\pi}n_{scans}$. The angular resolution is $d\theta = \dfrac{2\pi}{n_{scans}}$.
\item revolutions per second $\Omega$: This is measured in Hz and gives the length of each time step $d\tau = \dfrac{1}{n_{scans}\Omega}$
\item $n_{loops}$: The number of back and forth repeats of the sensor trajectory.
\end{itemize}
From these parameters the scanning direction ${^{A}\mathbf{d}(t)}$ is created. At each time $t$, ${^{A}\mathbf{d}(t)}$ is either a unit vector indicating the direction of measurement in the sensor frame, or has $\mathbf{0}$ magnitude, corresponding to when ${^{A}\mathbf{d}(t)}$ is outside the field of view and the sensor is not returning a measurement.

\begin{equation}
^{A}\mathbf{d}(t) =
	\begin{cases} 
	      \hfill \begin{bmatrix}
	      		\cos(-\theta + 2\pi t') \\
	      		-\sin(-\theta + 2\pi t') \\
	      		0
	      	\end{bmatrix}    \hfill & \text{ if $t' \leq \theta/pi$, $t' = k\delta\tau$ where $k \in \mathbb{N}$} \\
	      \hfill \mathbf{0} \hfill & \text{ if $t' > \theta/pi$, $t' \neq k\delta\tau$ where $k \in \mathbb{N}$} \\
	\end{cases} 
\end{equation}
where
\begin{equation}
t' = \mod(t,1/d\theta)\:d\theta
\end{equation}

Figure \ref{fig:scanningparameters} shows the frame $\{A\}$ fixed to the sensor and the scan direction ${^{A}\mathbf{d}(t)}$. At time $t' = 0$, the first scan direction ${^{A}\mathbf{d}_0}$ has an angular displacement of $-\theta$ about the $z$-axis from the forward facing $x$-direction. After each time step $d\tau$, the scan direction rotates by $d\theta$ about the $z$-axis. There are $n_{ranges}$ scan directions within the field of view of the sensor. The entire revolution is divided into $n_{scans}$ scan directions.
\input{Figures/fig_scanning}

To simulate range measurements, the scan direction is required in the inertial frame $\{F\}$. This is computed by multiplying with the screw matrix of the sensor.
\begin{equation}
	\begin{array}{lcl}
	{^{F}\mathbf{d'}(t)} & = & \mathbf{S}_s(t)\:{^{A}\mathbf{d'}(t)} \\
	& = & {^{F}_{F}\mathbf{S}^{}_{A}(t)}\:{^{A}\mathbf{d'}(t)}
	\end{array}
\end{equation}

\subsection{Environment Modelling}
\subsubsection{Motion}
The pose of each object represents the pose of its centre of mass. As described in section \ref{motion} the pose is computed with interpolation for the case of zero wrench, and numerical integration in the case of non-zero wrench.

\subsubsection{Rigid Objects}
The environment is represented with two rectangular prisms; the cube and a larger rectangular prism enclosing both the sensor and cube, to represent the background. These objects are modelled as an ordered set of 8 points in the inertial reference frame and an ordered set of 12 triangles formed by these points. Each triangle is represented by a set of 3 integers, indicating the index of the three points that make up its vertices.

The cube points in body frame $\{B\}$ are represented with the matrix ${^{B}\mathbf{P}}$.
\begin{equation}
	{^{B}\mathbf{P}} = \frac{1}{2}s
	\begin{bmatrix*}[r]
		-1  &  -1  &  -1  &  -1  &   1  &   1  &   1  &  \hspace{0.75em} 1 \\
		-1  &  -1  &   1  &   1  &  -1  &  -1  &   1  &  1 \\
		-1  &   1  &  -1  &   1  &  -1  &   1  &  -1  &  1 
	\end{bmatrix*}
\end{equation}
To represent these points in the inertial frame $\{F\}$, ${^{F}\mathbf{P}}$ is computed by rotating each point with the orientation quaternion of frame $\{B\}$ using equation \ref{quatrot} before adding the vector representing the translation of $\{B\}$ from $\{F\}$.

The triangles are represented with the matrix $\mathbf{T}$. Each triangle is represented by a row. The elements of these rows are the three vertices of the triangle and the index corresponds a point in ${^{F}\mathbf{P}}$.
\begin{equation}
	\mathbf{T} = 
	\begin{bmatrix}
	1 & 2 & 3 \\
	2 & 4 & 3 \\
    4 & 3 & 7 \\
    4 & 8 & 7 \\
    5 & 6 & 7 \\
    8 & 6 & 7 \\
    2 & 6 & 5 \\
    2 & 1 & 5 \\
    2 & 6 & 8 \\
    2 & 4 & 8 \\
    1 & 5 & 7 \\
    1 & 3 & 7
	\end{bmatrix}
\end{equation}

The points and triangles are shown in Figure \ref{fig:triangles}.
\input{Figures/fig_triangles}

\subsection{Measurement Modelling}
	\subsubsection{Range Computation}
	Given the screw matrix (in the inertial frame) and scan direction (in the body fixed frame) of the sensor, the position and scan direction in the inertial frame are determined.
	The distance to the nearest environment object from this point, along the scan direction is determined with the M{\"o}ller-Trumbore ray-triangle intersection algorithm, shown in Algorithm \ref{intersection}.
	
	EXPLANATION OF ALGORITHM \ref{intersectionalgorithm}
	
	*DIFFICULT TO VISUALISE. INCLUDE DIAGRAM SHOWING INTERSECTION OF RAY AND A SINGLE TRIANGLE
	Figure \ref{fig:raytriangle}
	\input{Figures/fig_raytriangle}

	\IncMargin{2em}
	\begin{algorithm}
	\DontPrintSemicolon
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\SetKwFunction{size}{size}
	\SetKwFunction{any}{any}
	\SetKwFunction{min}{min}
	\SetKwFunction{find}{find}
	\SetKwFunction{atan}{atan}
	
	\Input{$\mathbf{o}$ - ray origin\\
		   $\mathbf{d}$ - ray direction vector\\
		   $\mathbf{P}$ - cube in inertial frame \\
		   $\mathbf{T}$ - triangle matrix}
	\Output{$x$ - True/False - measurement corresponds to object\\
			$r$ - distance to object in m\\
			$\theta$ - incidence angle in rad\\
			$m$ - index of triangle hit}
	\Begin{
		\tcc{initialise outputs}
		$x \longleftarrow 0$\\
		$r \longleftarrow NaN$\\
		$\theta \longleftarrow NaN$\\
		$m \longleftarrow NaN$\\
		\tcc{triangle vertexes and edges}
		$\mathbf{V}_1 \longleftarrow \mathbf{P}[\mathbf{T}[:,1]]$\\
		$\mathbf{V}_2 \longleftarrow \mathbf{P}[\mathbf{T}[:,2]]$\\
		$\mathbf{V}_3 \longleftarrow \mathbf{P}[\mathbf{T}[:,3]]$\\
		$\mathbf{E}_1 \longleftarrow \mathbf{V}_2 - \mathbf{V}_1$\\
		$\mathbf{E}_2 \longleftarrow \mathbf{V}_3 - \mathbf{V}_1$\\
		$m = \size(\mathbf{V}_1,1)$\\
		$\mathbf{A} \longleftarrow \mathbf{o}_{[m \times 1]} - \mathbf{P}$\\
		\tcc{determinant}
		$\mathbf{B} \longleftarrow \mathbf{d}_{[m \times 1]} \times \mathbf{E}_2$ *(along dim 2)\\
		$\bm{\delta} \longleftarrow \mathbf{E}_1 \cdot \mathbf{B}$ *(along dim 2)\\		
		$\mathbf{y} \longleftarrow |\bm{\delta}|\leq\mathbf{0}$\\
		$\bm{\delta}[\mathbf{y}] \longleftarrow \mathbf{NaN}$\\
		\tcc{barycentric coordinates}
		$\mathbf{u} \longleftarrow (\mathbf{A} \cdot \mathbf{B})/\bm{\delta}$ *(along dim 2)\\
		$\mathbf{Q} \longleftarrow \mathbf{A} \times \mathbf{E}_1$ *(along dim 2)\\
		$\mathbf{v} \longleftarrow (\mathbf{d}_{[n \times 1]} \cdot \mathbf{Q})/\bm{\delta}$ *(along dim 2)\\
		$\mathbf{s} \longleftarrow (\mathbf{E}_2 \cdot \mathbf{Q})/\bm{\delta}$\\
		\tcc{intersection vector}	
		$\mathbf{z} \longleftarrow \mathbf{y} \textbf{ and } (\mathbf{u} \geq \mathbf{0}) 
		\textbf{ and } (\mathbf{v} \geq \mathbf{0}) \textbf{ and } (\mathbf{u}+\mathbf{v} \leq 
		\mathbf{0})$\\
		$\mathbf{x} \longleftarrow \mathbf{z} \textbf{ and } (\mathbf{s} \geq \mathbf{0})$\\
		\If{$\any(\mathbf{x})$}{
			$x \longleftarrow 1$\\
			$\mathbf{x}[\textbf{not }\mathbf{x}] \longleftarrow \mathbf{NaN}$\\
			$\mathbf{r} = \mathbf{s} \circ \mathbf{x}$\\
			$r = \min(\mathbf{r})$\\
			$m = \find(\mathbf{r} = r,1)$\\
			$\mathbf{e}_1 \longleftarrow \mathbf{E}_1[t,:]$\\ 
			$\mathbf{e}_2 \longleftarrow \mathbf{E}_2[t,:]$\\ 
			$\mathbf{n} = \mathbf{e}_1 \times \mathbf{e}_2$\\
			$\theta = \atan(|\mathbf{d}\times\mathbf{n}|,\mathbf{d}\cdot\mathbf{n})$ *atan2\\
			$\theta = \min(\theta,\pi-\theta)$
		}
	}
	\caption{M{\"o}ller-Trumbore ray-triangle intersection} \label{intersectionalgorithm}
	\end{algorithm}
	
	\subsubsection{Object Surface}
	random walk
	
	\subsubsection{Sensor Noise}
	Noise model depends on sensor used.
	\begin{equation}
		\hat{r}(t) = f_s(r(t),\theta(t),\phi(k))
	\end{equation}
	where $\theta(t)$ is incidence angle of measurement, $\phi$ is surface properties of object $k$ that was measured, $f$ is some function for noise model of particular sensor.
	
	For Hokuyo UBG-04LX-F01 used, noise model was measured experimentally:
	\begin{equation}
		f_{UBG}(r,\theta,\phi) = 
	\end{equation}
	
	
\subsection{Observer implementation}
	\subsubsection{Estimate: internal model}
		The state of the cube at each time step is estimated using the numerical integration method described in section \ref{integration}.
	
	\subsubsection{Identifying object/background}
		The variable $c$ indicates whether the range measurement is of the target object or the background. It is assumed that initially the sensor will be observing background, so $c_0 = FALSE$

		EXPLANATION OF ALGORITHM \ref{object}
		
		\IncMargin{2em}
		\begin{algorithm}
		\DontPrintSemicolon
		\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	
		\Input{$rangeAssumption$ - true/false\\
			   $differenceAssumption$ - true/false\\
			   $r_{max}$ - max range for cube\\
			   $\Delta_{max}$ - max diff between measurements of same object\\
			   $c$ - true/false - current measurement is of cube\\
			   $\mathbf{r}_{i+1}$ - distance to object at $t = i+1$\\
			   $\mathbf{r}_{i}$ - distance to object at $t = i$}
		\Output{$c$ - true/false}
		\Begin{				
			\If{$differenceAssumption$}{
				\If{$|\mathbf{r}_{i+1}-\mathbf{r}_{i}| > \Delta_{max}$}{
					$c = \mod(c + 1,2)$
				}
			}
			\If{$rangeAssumption$}{
				\If{$\mathbf{r}_{i+1} > r_{max}$}{
					$c = 0$
				}
			}
		}
		\caption{Target/background object separation} \label{object}
		\end{algorithm}
		
	\subsubsection{Update}
		\textbf{Input ranges:}\\
			Use ranges according to ordered sequence of indexes $a$. ie if all present - quadrilateral with points at corners:
			\begin{equation}
				a = \{ii,(ii-1),(ii-n_{scans}),(ii-1-n_{scans})\}
			\end{equation}
		
		\textbf{Orientation update:}\\
			Require at least 3 ranges in prediction and measurement. ie
			$|\hat{a}| \geq 3$ and $|\tilde{a}| \geq 3$
	
			From range and direction, compute coordinates of intersection\\
			\begin{equation}
				\mathbf{P}(a_k) = \mathbf{R}(a_k){^{F}\mathbf{N}(a_k)}
			\end{equation}
			\begin{equation}
				\mathbf{n} = [\mathbf{P}(a_2) - \mathbf{P}(a_1)] \times [\mathbf{p}(a_3) - \mathbf{p}(a_1)]
			\end{equation}
			Rotation axis from cross product of predicted and measured normals:\\
			\begin{equation}
				\mathbf{r}_{update} = \hat{\mathbf{n}} \times \tilde{\mathbf{n}}
			\end{equation}
			Rotation angle determined in settings. Table REF shows settings	
			
			figure~\ref{fig:orientation}
			\input{Figures/fig_orientationupdate}
			
		\textbf{Position update:}\\
			Require at least 1 range in prediction and measurement:\\
			$|\hat{a}| \geq 1$ \and $|\tilde{a}| \geq 1$\\
			Average to get $\hat{\bm{\mu}}_\mathbf{p}$ and $\tilde{\bm{\mu}}_\mathbf{p}$\\
			Scale translation vector - taking into account geometry of intersection points and sensor position.
			Get mean of all predicted and measured ranges = $\mu_{r}$. Need valid scan direction ie within FOV for $ii,ii-1,ii-1-n_{scans}$
			$\mathbf{p}_0 = \mathbf{p}_s(t) = {^{F}_{F}\mathbf{p}^{}_{A}(t)}$\\
			$\mathbf{p}_1 = \mu_{r}{^{F}\mathbf{s}(ii)}$\\	
			$\mathbf{p}_2 = \mu_{r}{^{F}\mathbf{s}(ii-1)}$\\
			$\mathbf{p}_3 = \mu_{r}{^{F}\mathbf{s}(ii-1-n_{scans})}$\\
			\begin{equation}
				\mathbf{p}_{update} = 
				\begin{bmatrix}
					\frac{1}{|\mathbf{p}_1-\mathbf{p}_0|} & 0 & 0\\
					0 & \frac{1}{|\mathbf{p}_2-\mathbf{p}_1|} & 0\\
					0 & 0 & \frac{1}{|\mathbf{p}_3-\mathbf{p}_2|}
				\end{bmatrix}
				(\tilde{\bm{\mu}}_\mathbf{p}-\hat{\bm{\mu}}_\mathbf{p})
			\end{equation}
			
			figure~\ref{fig:position}
			\input{Figures/fig_positionupdate}
			
		\textbf{Size update:}\\
			If different pattern $\hat{a} \neq \tilde{a}$\\
			At least 1 of each: $|\hat{a}| \geq 1$ \and $|\tilde{a}| \geq 1$\\
			\begin{equation}
				s_{update} = \mathbf{p}_{update} \cdot {^{F}\mathbf{d}(ii)}
			\end{equation}
			figure~\ref{fig:size1}
			\input{Figures/fig_sizeupdate1}
			
			If same pattern $\hat{a} = \tilde{a}$\\
			At least 1 of each: $|\hat{a}| \geq 1$ \and $|\tilde{a}| \geq 1$\\
			delta s = mean measured ranges - mean predicted ranges\\
			\begin{equation}
				s_{update} = \tilde{\mu}_r - \hat{\mu}_r
			\end{equation}	
				
			figure~\ref{fig:size2}
			\input{Figures/fig_sizeupdate2}
			
		\textbf{Update scheme:}\\
					
			Can update S,T,W of state (or perhaps some combination)\\
			Scale rotation axis, translation vector, delta s depending on if update is via S,T,W\\
			S: r to R, rotate orientation. Shift origin position with translation vector.\\
			T: add r to angular velocity. Add v to linear velocity.\\
			W: add r to angular acceleration. Add v to linear acceleration.

			TABLE - weights from config.
			\input{Tables/tab_updateweights}
			
\section{Results}
plots for different initial conditions \& kinds of motion

\begin{figure}
  \includegraphics[width=1.2\textwidth,trim = 0mm 0mm 0mm 0mm,clip]{./Figures/observer_error_stationary_orientation.jpg}
  \caption{Orientation correction}
\end{figure}

\begin{figure}
  \includegraphics[width=1.2\textwidth,trim = 0mm 0mm 0mm 0mm,clip]{./Figures/observer_error_stationary_orientation_size.jpg}
  \caption{Orientation \& size correction}
\end{figure}

\begin{figure}
  \includegraphics[width=1.2\textwidth,trim = 0mm 0mm 0mm 0mm,clip]{./Figures/observer_error_stationary_size.jpg}
  \caption{Size correction}
\end{figure}

\begin{figure}
  \includegraphics[width=1.2\textwidth,trim = 0mm 0mm 0mm 0mm,clip]{./Figures/observer_error_stationary_position.jpg}
  \caption{Position correction}
\end{figure}

	
